{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8a9bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install -U metapub pdfplumber pandas lxml requests beautifulsoup4 tqdm\n",
    "%pip -q install -U \"transformers>=4.38\" \"accelerate>=0.26\" \"bitsandbytes>=0.43\" \"safetensors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b7c70",
   "metadata": {},
   "source": [
    "## Step 1:建立目录与配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b129d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS_DIR: D:\\2026projects\\courses-HoNLP\\Annane_Sepsis_Corpus\n",
      "PAPERS_DIR: D:\\2026projects\\courses-HoNLP\\Annane_Sepsis_Corpus\\papers\n",
      "OUT_DIR: D:\\2026projects\\courses-HoNLP\\Annane_Sepsis_Corpus\\out\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "\n",
    "CORPUS_DIR = Path(\"Annane_Sepsis_Corpus\")\n",
    "PAPERS_DIR = CORPUS_DIR / \"papers\"\n",
    "OUT_DIR = CORPUS_DIR / \"out\"\n",
    "\n",
    "PAPERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CORPUS_DIR:\", CORPUS_DIR.resolve())\n",
    "print(\"PAPERS_DIR:\", PAPERS_DIR.resolve())\n",
    "print(\"OUT_DIR:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72e541",
   "metadata": {},
   "source": [
    "## Step 2: Topic 检索 + 下载 PDF + sections.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f466627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-10 10:27:32\u001b[0m \u001b[35mLAPTOP-4NMFDBK9\u001b[0m \u001b[34mmetapub.config[2552]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mNCBI_API_KEY was not set.\u001b[0m\n",
      "d:\\2026projects\\courses-HoNLP\\Tp5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: sepsis_diagnosis_criteria\n",
      "Query: (Annane D[Author] OR Djillali A[Author]) AND (sepsis OR \"septic shock\") AND (diagnosis OR criteria OR definition OR \"Sepsis-3\")\n",
      "PMIDs: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect sepsis_diagnosis_criteria:   0%|          | 0/141 [00:00<?, ?it/s]\u001b[32m2026-02-10 10:27:33\u001b[0m \u001b[35mLAPTOP-4NMFDBK9\u001b[0m \u001b[34mmetapub.findit[2552]\u001b[0m \u001b[1;30mINFO\u001b[0m FindIt Cache initialized at C:\\Users\\27858\\.cache\\findit.db\n",
      "collect sepsis_diagnosis_criteria:  18%|█▊        | 26/141 [00:14<01:06,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Done sepsis_diagnosis_criteria: 10/10\n",
      "\n",
      "============================================================\n",
      "Topic: sepsis_treatment_plan\n",
      "Query: (Annane D[Author] OR Djillali A[Author]) AND (sepsis OR \"septic shock\") AND (treatment OR therapy OR management OR guideline OR hydrocortisone)\n",
      "PMIDs: 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect sepsis_treatment_plan:  14%|█▎        | 31/228 [00:15<01:41,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Done sepsis_treatment_plan: 10/10\n",
      "\n",
      "============================================================\n",
      "Topic: steroid_sensitivity\n",
      "Query: (Annane D[Author] OR Djillali A[Author]) AND (sepsis OR \"septic shock\") AND (corticosteroid OR hydrocortisone OR steroid) AND (response OR sensitivity OR resistance)\n",
      "PMIDs: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collect steroid_sensitivity:  31%|███       | 23/75 [00:10<00:23,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Done steroid_sensitivity: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from metapub import PubMedFetcher, FindIt\n",
    "import requests\n",
    "import pdfplumber\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "fetcher = PubMedFetcher()\n",
    "\n",
    "TOPICS = {\n",
    "    \"sepsis_diagnosis_criteria\": '(Annane D[Author] OR Djillali A[Author]) AND (sepsis OR \"septic shock\") AND (diagnosis OR criteria OR definition OR \"Sepsis-3\")',\n",
    "    \"sepsis_treatment_plan\": '(Annane D[Author] OR Djillali A[Author]) AND (sepsis OR \"septic shock\") AND (treatment OR therapy OR management OR guideline OR hydrocortisone)',\n",
    "    \"steroid_sensitivity\": '(Annane D[Author] OR Djillali A[Author]) AND (sepsis OR \"septic shock\") AND (corticosteroid OR hydrocortisone OR steroid) AND (response OR sensitivity OR resistance)',\n",
    "}\n",
    "\n",
    "TARGET_PER_TOPIC = 10\n",
    "\n",
    "def pmid_search(query, retmax=200):\n",
    "    # metapub 支持 fetcher.pmids_for_query\n",
    "    pmids = fetcher.pmids_for_query(query, retmax=retmax)\n",
    "    return [str(x) for x in pmids]\n",
    "\n",
    "def is_pdf_bytes(b: bytes) -> bool:\n",
    "    return b[:4] == b\"%PDF\"\n",
    "\n",
    "def download_pdf_from_url(url, out_path: Path, timeout=60):\n",
    "    r = requests.get(url, timeout=timeout, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    r.raise_for_status()\n",
    "    data = r.content\n",
    "    if not is_pdf_bytes(data):\n",
    "        return False\n",
    "    out_path.write_bytes(data)\n",
    "    return True\n",
    "\n",
    "def find_pdf_url(pmid: str):\n",
    "    try:\n",
    "        src = FindIt(pmid)\n",
    "        if src.url:\n",
    "            return src.url\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pdf_to_text(pdf_path: Path, max_pages=10):\n",
    "    text = []\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        pages = pdf.pages[:max_pages]\n",
    "        for p in pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                text.append(t)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def sectionize_simple(full_text: str):\n",
    "    # 非完美：先给你一个可稳定跑的最简分段（Abstract/Method/Conclusion）\n",
    "    # 后续你可以升级为更强的 heading detector\n",
    "    t = re.sub(r\"\\s+\", \" \", full_text).strip()\n",
    "    sections = {\"Abstract\":\"\", \"Method\":\"\", \"Conclusion\":\"\"}\n",
    "\n",
    "    # 粗规则：按关键词切\n",
    "    m = re.search(r\"\\b(METHODS?|PATIENTS?|DESIGN|SETTING)\\b\", t, flags=re.I)\n",
    "    c = re.search(r\"\\b(CONCLUSION(S)?|RESULTS)\\b\", t, flags=re.I)\n",
    "\n",
    "    if m:\n",
    "        sections[\"Abstract\"] = t[:m.start()].strip()\n",
    "        if c and c.start() > m.start():\n",
    "            sections[\"Method\"] = t[m.start():c.start()].strip()\n",
    "            sections[\"Conclusion\"] = t[c.start():].strip()\n",
    "        else:\n",
    "            sections[\"Method\"] = t[m.start():].strip()\n",
    "    else:\n",
    "        sections[\"Abstract\"] = t[:4000]  # fallback\n",
    "\n",
    "    return sections\n",
    "\n",
    "def save_paper(pmid: str, topic: str, pdf_bytes_ok: bool, pdf_path: Path, sections: dict, meta: dict):\n",
    "    paper_dir = PAPERS_DIR / f\"PMID_{pmid}\"\n",
    "    paper_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if pdf_bytes_ok:\n",
    "        # 已下载到 pdf_path（先写到 paper_dir）\n",
    "        (paper_dir / \"paper.pdf\").write_bytes(pdf_path.read_bytes())\n",
    "\n",
    "    sec_obj = {\n",
    "        \"pmid\": pmid,\n",
    "        \"topic\": topic,\n",
    "        \"pdf_path\": str(paper_dir / \"paper.pdf\") if pdf_bytes_ok else None,\n",
    "        \"sectionizer\": \"simple_pdfplumber\",\n",
    "        \"sections\": sections,\n",
    "        \"meta\": meta\n",
    "    }\n",
    "    (paper_dir / \"sections.json\").write_text(json.dumps(sec_obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return paper_dir\n",
    "\n",
    "def already_ready(paper_dir: Path):\n",
    "    return (paper_dir/\"paper.pdf\").exists() and (paper_dir/\"sections.json\").exists()\n",
    "\n",
    "tmp_pdf = OUT_DIR / \"tmp_download.pdf\"\n",
    "\n",
    "for topic, query in TOPICS.items():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Topic:\", topic)\n",
    "    print(\"Query:\", query)\n",
    "\n",
    "    pmids = pmid_search(query, retmax=500)\n",
    "    print(\"PMIDs:\", len(pmids))\n",
    "\n",
    "    saved = 0\n",
    "    for pmid in tqdm(pmids[:500], desc=f\"collect {topic}\"):\n",
    "        paper_dir = PAPERS_DIR / f\"PMID_{pmid}\"\n",
    "        if already_ready(paper_dir):\n",
    "            saved += 1\n",
    "            if saved >= TARGET_PER_TOPIC:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        url = find_pdf_url(pmid)\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        ok_pdf = False\n",
    "        try:\n",
    "            ok_pdf = download_pdf_from_url(url, tmp_pdf)\n",
    "        except Exception:\n",
    "            ok_pdf = False\n",
    "\n",
    "        if not ok_pdf:\n",
    "            continue\n",
    "\n",
    "        # 抽文本并分段\n",
    "        ft = pdf_to_text(tmp_pdf, max_pages=12)\n",
    "        secs = sectionize_simple(ft)\n",
    "        meta = {\"findit_url\": url}\n",
    "\n",
    "        save_paper(pmid, topic, True, tmp_pdf, secs, meta)\n",
    "        saved += 1\n",
    "        print(f\"✅ {topic}: PMID {pmid} saved ({saved}/{TARGET_PER_TOPIC})\")\n",
    "        if saved >= TARGET_PER_TOPIC:\n",
    "            break\n",
    "\n",
    "    print(f\"==> Done {topic}: {saved}/{TARGET_PER_TOPIC}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92df7a",
   "metadata": {},
   "source": [
    "## 读取一篇 READY paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444cb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_ready_paper():\n",
    "    for p in sorted(PAPERS_DIR.glob(\"PMID_*\")):\n",
    "        sec = p / \"sections.json\"\n",
    "        pdf = p / \"paper.pdf\"\n",
    "        if not (sec.exists() and pdf.exists()):\n",
    "            continue\n",
    "        obj = json.loads(sec.read_text(encoding=\"utf-8\"))\n",
    "        secs = obj.get(\"sections\") or {}\n",
    "        if any((v or \"\").strip() for v in secs.values()):\n",
    "            return p, obj\n",
    "    raise RuntimeError(\"No READY paper found\")\n",
    "\n",
    "def clean_pdf_text(t: str) -> str:\n",
    "    t = t.replace(\"\\x00\", \" \")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # 常见页眉/引用信息/作者单位行：直接删掉（保守）\n",
    "    drop_patterns = [\n",
    "        r\"^critical care.*$\",                 # 期刊页眉\n",
    "        r\"^intensive care med.*$\",            # 期刊页眉\n",
    "        r\"^doi\\s*:\\s*\\S+.*$\",                 # DOI 行\n",
    "        r\"^vol\\s*\\d+.*$\",                     # Vol 行\n",
    "        r\"^review.*clinical review.*$\",       # Review 标题\n",
    "        r\"^guidelines.*$\",                    # Guidelines 标题\n",
    "        r\"^copyright.*$\",                     # copyright\n",
    "        r\"^open access.*$\",                   # open access\n",
    "    ]\n",
    "\n",
    "    lines = [x.strip() for x in t.split(\"\\n\") if x.strip()]\n",
    "    kept = []\n",
    "    for ln in lines:\n",
    "        low = ln.lower()\n",
    "        if any(re.match(p, low) for p in drop_patterns):\n",
    "            continue\n",
    "        # 过短、像作者列表的也跳过\n",
    "        if len(ln) < 25:\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "\n",
    "    return \" \".join(kept)\n",
    "\n",
    "def paper_text_from_sections(obj, max_chars=12000):\n",
    "    \"\"\"\n",
    "    把 sections.json 的内容拼成：\n",
    "    [ABSTRACT]\n",
    "    ...\n",
    "    [METHOD]\n",
    "    ...\n",
    "    这样 parse_section_blocks 才能识别。\n",
    "    \"\"\"\n",
    "    secs = obj.get(\"sections\") or {}\n",
    "    parts = []\n",
    "\n",
    "    # 尽量多给一些常见章节（不止 Method/Conclusion）\n",
    "    order = [\n",
    "        \"Abstract\", \"Introduction\", \"Background\",\n",
    "        \"Methods\", \"Method\", \"Patients\", \"Materials\",\n",
    "        \"Results\", \"Discussion\", \"Conclusion\",\n",
    "        \"Recommendation\", \"Guideline\"\n",
    "    ]\n",
    "\n",
    "    for k in order:\n",
    "        t = (secs.get(k) or \"\").strip()\n",
    "        if t:\n",
    "            parts.append(f\"[{k.upper()}]\\n{t}\")  # ✅ header 单独一行 + 换行\n",
    "\n",
    "    text = \"\\n\\n\".join(parts).strip()\n",
    "\n",
    "    # 兜底：如果 sections 为空，就用 abstract 字段\n",
    "    if not text:\n",
    "        abs_ = (obj.get(\"abstract\") or \"\").strip()\n",
    "        if abs_:\n",
    "            text = f\"[ABSTRACT]\\n{abs_}\"\n",
    "\n",
    "    return text[:max_chars]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63444ca4",
   "metadata": {},
   "source": [
    "## 加载 Gemma（4-bit，避免 kernel crash）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e2a9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 164/164 [00:09<00:00, 16.53it/s, Materializing param=model.norm.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Gemma OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK 是一个词，表示同意或接受。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\27858\\hf_gemma_2b_it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    low_cpu_mem_usage=True,   # ✅ 更省内存\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded Gemma OK.\")\n",
    "\n",
    "\n",
    "def gemma_generate(prompt: str, max_new_tokens=160, temperature=0.2, max_input_tokens=1024):\n",
    "\n",
    "    # gemma-it 用 chat template\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        enc = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = enc.input_ids if hasattr(enc, \"input_ids\") else enc\n",
    "    else:\n",
    "        enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "\n",
    "    if input_ids.shape[-1] > max_input_tokens:\n",
    "        input_ids = input_ids[:, -max_input_tokens:]\n",
    "\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature is not None and temperature > 0),\n",
    "            temperature=float(temperature) if temperature else 1.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    gen_ids = out[0, input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "print(gemma_generate(\"输出一个词:OK\", max_new_tokens=20, temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57ce25",
   "metadata": {},
   "source": [
    "核心修复：句子编号法（保证 quote 100% 来自文章）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 基础工具：句子切分 + 去噪\n",
    "# -----------------------------\n",
    "BAD_PATTERNS = [\n",
    "    r\"\\bdoi\\b\", r\"\\bvol\\b\", r\"\\bissue\\b\", r\"\\bpages?\\b\", r\"copyright\",\n",
    "    r\"\\bpmid\\b\", r\"\\bpmc\\b\", r\"http[s]?://\", r\"@\", r\"conflict of interest\",\n",
    "    r\"funding\", r\"affiliation\", r\"corresponding author\"\n",
    "]\n",
    "\n",
    "\"\"\"def is_noise_sentence(s: str) -> bool:\n",
    "    ss = (s or \"\").strip().lower()\n",
    "    if len(ss) < 35:\n",
    "        return True\n",
    "    if any(re.search(p, ss) for p in BAD_PATTERNS):\n",
    "        return True\n",
    "    # 数字太多往往是引用/表格/作者信息\n",
    "    if sum(ch.isdigit() for ch in ss) > 18:\n",
    "        return True\n",
    "    return False\"\"\"\n",
    "\n",
    "NOISE_PATTERNS = [\n",
    "    r\"\\bissn\\b\", r\"\\bdoi\\b\", r\"\\bvol\\.?\\b\", r\"\\bissue\\b\",\n",
    "    r\"author manuscript\", r\"published in final edited form\",\n",
    "    r\"springer\", r\"cochrane\", r\"wiley\", r\"elsevier\",\n",
    "    r\"all rights reserved\", r\"copyright\",\n",
    "    r\"\\btable of contents\\b\", r\"\\bcontents\\b\",\n",
    "    r\"guidelines committee\", r\"surviving sepsis campaign\",  # 这类常变成“标题/名单”\n",
    "    r\"\\babstract\\b.*\\bonline\\b\",  # 类似“Online ISSN ... Abstract ...”\n",
    "]\n",
    "\n",
    "def is_noise_sentence(s: str) -> bool:\n",
    "    if not s:\n",
    "        return True\n",
    "    t = s.strip()\n",
    "    if len(t) < 40:\n",
    "        return True\n",
    "    low = t.lower()\n",
    "    # 作者名单/机构名单：逗号密度很高且缺少谓词\n",
    "    if low.count(\",\") >= 6 and not any(w in low for w in [\"increase\", \"reduce\", \"associated\", \"defined\", \"recommend\", \"should\", \"mortality\", \"diagnos\", \"treat\"]):\n",
    "        return True\n",
    "    for pat in NOISE_PATTERNS:\n",
    "        if re.search(pat, low):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def split_sentences_en(text: str):\n",
    "    \"\"\"英文为主的句子切分，尽量稳。\"\"\"\n",
    "    t = (text or \"\").replace(\"\\x00\", \" \")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    if not t:\n",
    "        return []\n",
    "    # 先按换行拆块，再按句末符拆\n",
    "    blocks = [b.strip() for b in re.split(r\"[\\r\\n]+\", t) if b.strip()]\n",
    "    base = \" \".join(blocks)\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+|(?<=;)\\s+\", base)\n",
    "    sents = [s.strip() for s in sents if s and len(s.strip()) > 20]\n",
    "    sents = [s for s in sents if not is_noise_sentence(s)]\n",
    "\n",
    "    return sents\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 从你现有 text（包含 [SECTION]）中解析出 block\n",
    "# -----------------------------\n",
    "def parse_section_blocks(text: str):\n",
    "    \"\"\"\n",
    "    兼容两种格式：\n",
    "    1) [METHOD]   (单独一行)\n",
    "       patients...\n",
    "    2) [METHOD] patients...  (同一行)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    cur_sec = \"Unknown\"\n",
    "    buf = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buf, cur_sec\n",
    "        if buf:\n",
    "            blocks.append((cur_sec.title(), \"\\n\".join(buf).strip()))\n",
    "            buf = []\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.rstrip()\n",
    "\n",
    "        # ✅ 情况1/2：行首出现 [XXX]\n",
    "        m = re.match(r\"^\\[(.+?)\\]\\s*(.*)$\", line.strip())\n",
    "        if m:\n",
    "            flush()\n",
    "            cur_sec = m.group(1).strip()\n",
    "            rest = (m.group(2) or \"\").strip()\n",
    "            if rest:\n",
    "                buf.append(rest)  # ✅ 把同一行剩余正文也加入\n",
    "        else:\n",
    "            if line.strip():\n",
    "                buf.append(line)\n",
    "\n",
    "    flush()\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 构造“编号句子列表”，按 section 权重优先\n",
    "# -----------------------------\n",
    "SEC_WEIGHT = {\n",
    "    \"Abstract\": 5,\n",
    "    \"Conclusion\": 5,\n",
    "    \"Results\": 4,\n",
    "    \"Discussion\": 3,\n",
    "    \"Recommendation\": 5,\n",
    "    \"Guideline\": 5,\n",
    "    \"Introduction\": 2,\n",
    "    \"Background\": 2,\n",
    "    \"Method\": 1,\n",
    "    \"Methods\": 1,\n",
    "    \"Patients\": 1,\n",
    "    \"Materials\": 1,\n",
    "    \"Unknown\": 2,\n",
    "}\n",
    "\n",
    "def build_indexed_sentences_by_section(text: str, max_sents=80):\n",
    "    blocks = parse_section_blocks(text)\n",
    "    if not blocks:\n",
    "        return [], \"\"\n",
    "\n",
    "    cand = []  # (weight, section, sent)\n",
    "    for sec, sec_text in blocks:\n",
    "        w = SEC_WEIGHT.get(sec, 2)\n",
    "        for s in split_sentences_en(sec_text):\n",
    "            if is_noise_sentence(s):\n",
    "                continue\n",
    "            cand.append((w, sec, s))\n",
    "\n",
    "    # 如果过滤太狠 -> 降级：不做噪声过滤\n",
    "    if len(cand) < 8:\n",
    "        cand = []\n",
    "        for sec, sec_text in blocks:\n",
    "            w = SEC_WEIGHT.get(sec, 2)\n",
    "            for s in split_sentences_en(sec_text):\n",
    "                if len((s or \"\").strip()) < 25:\n",
    "                    continue\n",
    "                cand.append((w, sec, s))\n",
    "\n",
    "    cand.sort(key=lambda x: x[0], reverse=True)\n",
    "    cand = cand[:max_sents]\n",
    "\n",
    "    # 句子列表（保留 section 信息）\n",
    "    sents = [{\"sec\": sec, \"sent\": sent} for _, sec, sent in cand]\n",
    "\n",
    "    indexed_lines = []\n",
    "    for i, item in enumerate(sents, start=1):\n",
    "        indexed_lines.append(f\"{i}. [{item['sec'].upper()}] {item['sent']}\")\n",
    "    indexed = \"\\n\".join(indexed_lines)\n",
    "\n",
    "    return sents, indexed\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 让 Gemma 只输出固定文本格式（避免 JSON 崩）\n",
    "# -----------------------------\n",
    "def build_quote_pick_prompt(paper_id, topic, indexed_sents_text):\n",
    "    return f\"\"\"\n",
    "你是医学论证结构抽取助手。\n",
    "\n",
    "请从下面“编号句子”里挑选：\n",
    "- CLAIM：1句话，必须是观点/结论/诊断依据/治疗建议相关，不要选作者/期刊/DOI/引用列表。\n",
    "- E1/E2/E3：3句话，分别作为支持或补充 CLAIM 的证据（尽量来自不同 section）。\n",
    "\n",
    "严格按照下面格式输出（不要输出其它内容，不要JSON，不要解释）：\n",
    "CLAIM: <编号>\n",
    "E1: <编号>\n",
    "E2: <编号>\n",
    "E3: <编号>\n",
    "\n",
    "paper_id={paper_id}\n",
    "topic={topic}\n",
    "\n",
    "编号句子：\n",
    "{indexed_sents_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "def parse_pick_output(raw: str, n_sents: int):\n",
    "    \"\"\"\n",
    "    解析：\n",
    "      CLAIM: 12\n",
    "      E1: 5\n",
    "      E2: 8\n",
    "      E3: 3\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        raise ValueError(\"Empty model output\")\n",
    "    def grab(tag):\n",
    "        m = re.search(rf\"{tag}\\s*:\\s*(\\d+)\", raw, flags=re.IGNORECASE)\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    claim_id = grab(\"CLAIM\")\n",
    "    e1 = grab(\"E1\")\n",
    "    e2 = grab(\"E2\")\n",
    "    e3 = grab(\"E3\")\n",
    "\n",
    "    # 兜底\n",
    "    claim_id = claim_id or 1\n",
    "    ev_ids = [e for e in [e1, e2, e3] if e is not None]\n",
    "    if len(ev_ids) < 3:\n",
    "        # 补齐\n",
    "        for i in range(2, 2 + (3 - len(ev_ids))):\n",
    "            ev_ids.append(i)\n",
    "\n",
    "    # 边界修正\n",
    "    claim_id = max(1, min(claim_id, n_sents))\n",
    "    ev_ids = [max(1, min(i, n_sents)) for i in ev_ids]\n",
    "    # 去重并保证3个\n",
    "    ev_ids = list(dict.fromkeys(ev_ids))\n",
    "    i = 1\n",
    "    while len(ev_ids) < 3 and i <= n_sents:\n",
    "        if i != claim_id and i not in ev_ids:\n",
    "            ev_ids.append(i)\n",
    "        i += 1\n",
    "    while len(ev_ids) < 3:\n",
    "        ev_ids.append(ev_ids[-1])\n",
    "\n",
    "    # 避免 evidence 跟 claim 一样\n",
    "    ev_ids = [i for i in ev_ids if i != claim_id]\n",
    "    while len(ev_ids) < 3:\n",
    "        ev_ids.append(1 if claim_id != 1 else 2)\n",
    "\n",
    "    return claim_id, ev_ids[:3]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 主函数：pick_quotes_by_ids（你后面流程直接用它）\n",
    "# -----------------------------\n",
    "def pick_quotes_by_ids(paper_id, topic, text, max_sents=80):\n",
    "    sents, indexed = build_indexed_sentences_by_section(text, max_sents=max_sents)\n",
    "\n",
    "    # 兜底：没句子\n",
    "    if not sents:\n",
    "        fallback = (text or \"\").strip()[:500] or \"No usable text extracted from paper sections.\"\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"topic\": topic,\n",
    "            \"claim_quote\": fallback,\n",
    "            \"evidence_quotes\": [fallback, fallback, fallback],\n",
    "            \"debug\": {\"reason\": \"no_sents\"}\n",
    "        }\n",
    "\n",
    "    # ✅ 不再调用 Gemma 选编号：直接按权重后的顺序取\n",
    "    # 由于 build_indexed_sentences_by_section 已经按 section 权重排序，\n",
    "    # 前面更可能是 Abstract/Conclusion/Results/Discussion。\n",
    "    claim = sents[0]\n",
    "\n",
    "    evidences = []\n",
    "    for item in sents[1:]:\n",
    "        # 尽量来自不同 section（更像你要的“链条”）\n",
    "        if item[\"sec\"] != claim[\"sec\"] and len(evidences) < 3:\n",
    "            evidences.append(item)\n",
    "        if len(evidences) >= 3:\n",
    "            break\n",
    "\n",
    "    # 如果还不够 3 条，就直接补齐\n",
    "    i = 1\n",
    "    while len(evidences) < 3 and i < len(sents):\n",
    "        evidences.append(sents[i])\n",
    "        i += 1\n",
    "\n",
    "    claim_quote = f\"[{claim['sec'].upper()}] {claim['sent']}\"\n",
    "    evidence_quotes = [f\"[{e['sec'].upper()}] {e['sent']}\" for e in evidences[:3]]\n",
    "\n",
    "    return {\n",
    "        \"paper_id\": paper_id,\n",
    "        \"topic\": topic,\n",
    "        \"claim_quote\": claim_quote,\n",
    "        \"evidence_quotes\": evidence_quotes,\n",
    "        \"debug\": {\"n_sents\": len(sents), \"mode\": \"heuristic\"}\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) （可选）JSON 提取工具：给后续 CE 用\n",
    "#     如果你后面还需要模型输出 JSON，可以保留它\n",
    "# -----------------------------\n",
    "def extract_json_anywhere(text: str):\n",
    "    if not text:\n",
    "        raise ValueError(\"Empty model output\")\n",
    "\n",
    "    t = text.strip().replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    start = t.find(\"{\")\n",
    "    if start == -1:\n",
    "        raise ValueError(\"No '{' found in output.\")\n",
    "\n",
    "    depth = 0\n",
    "    end = None\n",
    "    for i in range(start, len(t)):\n",
    "        ch = t[i]\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                end = i\n",
    "                break\n",
    "\n",
    "    if end is None:\n",
    "        cand = t[start:]\n",
    "        missing = 0\n",
    "        for ch in cand:\n",
    "            if ch == \"{\":\n",
    "                missing += 1\n",
    "            elif ch == \"}\":\n",
    "                missing -= 1\n",
    "        if missing > 0:\n",
    "            cand = cand + (\"}\" * missing)\n",
    "        return json.loads(cand)\n",
    "\n",
    "    blob = t[start:end+1]\n",
    "    return json.loads(blob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dfc91",
   "metadata": {},
   "source": [
    "## Step3：第二阶段：把 quote 变成 claim/evidence 的可视化结构（生成 argument_tree）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13558e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "\n",
    "def extract_first_json_object(raw: str):\n",
    "    \"\"\"从输出中提取第一段 JSON 对象（允许前后夹杂文本/markdown）\"\"\"\n",
    "    if not raw:\n",
    "        raise ValueError(\"empty output\")\n",
    "    t = raw.strip().replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    # 去掉 ```json ``` 包裹\n",
    "    t = re.sub(r\"^```(?:json)?\\s*\", \"\", t)\n",
    "    t = re.sub(r\"\\s*```$\", \"\", t)\n",
    "\n",
    "    start = t.find(\"{\")\n",
    "    if start == -1:\n",
    "        raise ValueError(\"no { found\")\n",
    "\n",
    "    depth = 0\n",
    "    end = None\n",
    "    for i in range(start, len(t)):\n",
    "        if t[i] == \"{\":\n",
    "            depth += 1\n",
    "        elif t[i] == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                end = i\n",
    "                break\n",
    "\n",
    "    if end is None:\n",
    "        # 缺右括号：补齐\n",
    "        cand = t[start:]\n",
    "        missing = 0\n",
    "        for ch in cand:\n",
    "            if ch == \"{\": missing += 1\n",
    "            elif ch == \"}\": missing -= 1\n",
    "        if missing > 0:\n",
    "            cand += \"}\" * missing\n",
    "        return json.loads(cand)\n",
    "\n",
    "    return json.loads(t[start:end+1])\n",
    "\n",
    "def normalize_ce_obj(obj: dict, paper_id: str, topic: str):\n",
    "    \"\"\"\n",
    "    统一成：\n",
    "    {\n",
    "      \"paper_id\": \"...\",\n",
    "      \"topic\": \"...\",\n",
    "      \"claim_text\": \"...\",\n",
    "      \"evidence_summaries\": [\"...\", \"...\", \"...\"]\n",
    "    }\n",
    "    允许两种输入：\n",
    "    A) {\"paper_id\",\"topic\",\"claim_text\",\"evidence_summaries\":[...]}\n",
    "    B) {\"CLAIM\":\"...\",\"E1\":\"...\",\"E2\":\"...\",\"E3\":\"...\"}  (你现在遇到的)\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"CE output is not dict\")\n",
    "\n",
    "    # 情况B：CLAIM/E1/E2/E3\n",
    "    if \"CLAIM\" in obj or \"E1\" in obj:\n",
    "        claim = (obj.get(\"CLAIM\") or obj.get(\"claim\") or \"\").strip()\n",
    "        e1 = (obj.get(\"E1\") or \"\").strip()\n",
    "        e2 = (obj.get(\"E2\") or \"\").strip()\n",
    "        e3 = (obj.get(\"E3\") or \"\").strip()\n",
    "        ev = [x for x in [e1, e2, e3] if x]\n",
    "        while len(ev) < 3:\n",
    "            ev.append(\"Supports the claim based on the quoted sentence.\")\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"topic\": topic,\n",
    "            \"claim_text\": claim or \"Claim.\",\n",
    "            \"evidence_summaries\": ev[:3],\n",
    "        }\n",
    "\n",
    "    # 情况A：claim_text/evidence_summaries\n",
    "    claim = (obj.get(\"claim_text\") or obj.get(\"claim\") or \"\").strip()\n",
    "    ev = obj.get(\"evidence_summaries\") or obj.get(\"evidence\") or []\n",
    "    if isinstance(ev, str):\n",
    "        ev = [ev]\n",
    "    if not isinstance(ev, list):\n",
    "        ev = []\n",
    "    ev = [str(x).strip() for x in ev if str(x).strip()]\n",
    "    while len(ev) < 3:\n",
    "        ev.append(\"Supports the claim based on the quoted sentence.\")\n",
    "\n",
    "    return {\n",
    "        \"paper_id\": obj.get(\"paper_id\", paper_id),\n",
    "        \"topic\": obj.get(\"topic\", topic),\n",
    "        \"claim_text\": claim or \"Claim.\",\n",
    "        \"evidence_summaries\": ev[:3],\n",
    "    }\n",
    "\n",
    "def parse_ce_text_fallback(raw: str):\n",
    "    \"\"\"\n",
    "    如果模型没输出 JSON，而是输出：\n",
    "    CLAIM: ...\n",
    "    E1: ...\n",
    "    E2: ...\n",
    "    E3: ...\n",
    "    就用这个解析\n",
    "    \"\"\"\n",
    "    lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
    "    claim = \"\"\n",
    "    ev = []\n",
    "    for l in lines:\n",
    "        if l.upper().startswith(\"CLAIM:\"):\n",
    "            claim = l.split(\":\", 1)[1].strip()\n",
    "        elif re.match(r\"^E\\d+\\s*:\", l.upper()):\n",
    "            ev.append(l.split(\":\", 1)[1].strip())\n",
    "    while len(ev) < 3:\n",
    "        ev.append(\"Supports the claim based on the quoted sentence.\")\n",
    "    return claim, ev[:3]\n",
    "\n",
    "def build_ce_from_quotes_prompt(paper_id, topic, claim_quote, evidence_quotes):\n",
    "    return f\"\"\"\n",
    "你是医学论证结构抽取助手。\n",
    "\n",
    "下面给你：\n",
    "- claim 原文句\n",
    "- 3条 evidence 原文句\n",
    "\n",
    "请你只输出“严格 JSON”，不要输出解释/markdown。\n",
    "必须使用下面键名（大小写必须一致）：\n",
    "{{\n",
    "  \"CLAIM\": \"用一句话概括 claim\",\n",
    "  \"E1\": \"用一句话概括 evidence1 如何支持/反驳\",\n",
    "  \"E2\": \"用一句话概括 evidence2 如何支持/反驳\",\n",
    "  \"E3\": \"用一句话概括 evidence3 如何支持/反驳\"\n",
    "}}\n",
    "\n",
    "claim 原文：\n",
    "\\\"\\\"\\\"{claim_quote}\\\"\\\"\\\"\n",
    "\n",
    "evidence 原文：\n",
    "1) \\\"\\\"\\\"{evidence_quotes[0]}\\\"\\\"\\\"\n",
    "2) \\\"\\\"\\\"{evidence_quotes[1]}\\\"\\\"\\\"\n",
    "3) \\\"\\\"\\\"{evidence_quotes[2]}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "def gemma_json_call_ce(prompt, max_new_tokens=220, temperature=0.2, retries=3):\n",
    "    \"\"\"\n",
    "    最终统一返回：\n",
    "      {\"claim_text\": str, \"evidence_summaries\": [e1,e2,e3]}\n",
    "    兼容模型输出：\n",
    "      A) JSON: {\"CLAIM\": \"...\", \"E1\": \"...\", ...}（哪怕不完全合法也尽量抠）\n",
    "      B) JSON: {\"claim_text\": \"...\", \"evidence_summaries\": [...]}\n",
    "      C) 文本: CLAIM: ... / E1: ... / E2: ... / E3: ...\n",
    "    \"\"\"\n",
    "    last = None\n",
    "\n",
    "    def _normalize(claim_text, ev_list):\n",
    "        claim_text = (claim_text or \"\").strip()\n",
    "        ev_list = [str(x).strip() for x in (ev_list or []) if str(x).strip()]\n",
    "\n",
    "        if not claim_text:\n",
    "            claim_text = \"Claim.\"\n",
    "\n",
    "        while len(ev_list) < 3:\n",
    "            ev_list.append(\"Supports the claim.\")\n",
    "        return {\"claim_text\": claim_text, \"evidence_summaries\": ev_list[:3]}\n",
    "\n",
    "    def _regex_grab(raw: str):\n",
    "        \"\"\"\n",
    "        关键：从类似\n",
    "          { \"CLAIM\": \"...\", \"E1\": \"...\", ... }\n",
    "        的输出里直接抠，不依赖 JSON 合法性。\n",
    "        \"\"\"\n",
    "        t = raw.replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        def grab(key):\n",
    "            # 支持 \"CLAIM\": \"...\" 或 CLAIM: ...\n",
    "            # 尽量少吃：非贪婪匹配到下一个 \"E2\" 或行尾\n",
    "            # 先抓引号型\n",
    "            m = re.search(rf'\"{key}\"\\s*:\\s*\"(.+?)\"\\s*(,|\\n|}})', t, flags=re.S)\n",
    "            if m:\n",
    "                return m.group(1).strip()\n",
    "\n",
    "            # 再抓无引号型（CLAIM: xxx）\n",
    "            m = re.search(rf'^{key}\\s*:\\s*(.+)$', t, flags=re.I|re.M)\n",
    "            if m:\n",
    "                return m.group(1).strip()\n",
    "\n",
    "            return \"\"\n",
    "\n",
    "        claim = grab(\"CLAIM\")\n",
    "        e1 = grab(\"E1\")\n",
    "        e2 = grab(\"E2\")\n",
    "        e3 = grab(\"E3\")\n",
    "\n",
    "        ev = [x for x in [e1, e2, e3] if x]\n",
    "        if claim or ev:\n",
    "            return _normalize(claim, ev)\n",
    "\n",
    "        return None\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        raw = gemma_generate(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        last = raw\n",
    "        print(f\"[CE attempt {attempt}] head:\", raw[:120].replace(\"\\n\", \" \"))\n",
    "\n",
    "        # ---- 1) 尝试严格/半严格 JSON ----\n",
    "        obj = None\n",
    "        try:\n",
    "            obj = extract_json_anywhere(raw)  # 你已有\n",
    "        except Exception:\n",
    "            obj = None\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            # 已经是标准字段\n",
    "            if \"claim_text\" in obj and \"evidence_summaries\" in obj:\n",
    "                return _normalize(obj.get(\"claim_text\"), obj.get(\"evidence_summaries\"))\n",
    "\n",
    "            # CLAIM/E1/E2/E3 字段\n",
    "            claim = obj.get(\"CLAIM\") or obj.get(\"claim\") or obj.get(\"Claim\")\n",
    "            ev = []\n",
    "            for k in [\"E1\",\"E2\",\"E3\",\"e1\",\"e2\",\"e3\"]:\n",
    "                if k in obj and str(obj[k]).strip():\n",
    "                    ev.append(str(obj[k]).strip())\n",
    "            if claim or ev:\n",
    "                return _normalize(claim, ev)\n",
    "\n",
    "        # ---- 2) JSON不合法也能抠：regex 抠 CLAIM/E1/E2/E3 ----\n",
    "        grabbed = _regex_grab(raw)\n",
    "        if grabbed is not None:\n",
    "            return grabbed\n",
    "\n",
    "        # ---- 3) 纯文本 CLAIM:/E1:... 解析 ----\n",
    "        claim_text = \"\"\n",
    "        ev = []\n",
    "        for line in raw.splitlines():\n",
    "            l = line.strip()\n",
    "            if not l:\n",
    "                continue\n",
    "            if l.upper().startswith(\"CLAIM:\"):\n",
    "                claim_text = l.split(\":\", 1)[1].strip()\n",
    "            elif re.match(r\"^E[1-3]:\", l, flags=re.I):\n",
    "                ev.append(l.split(\":\", 1)[1].strip())\n",
    "\n",
    "        if claim_text or ev:\n",
    "            return _normalize(claim_text, ev)\n",
    "\n",
    "        # ---- 4) 重试：更强提示 + 降温 ----\n",
    "        prompt = (\n",
    "            \"只输出下面格式（不要解释，不要总结，不要markdown）：\\n\"\n",
    "            \"CLAIM: ...\\nE1: ...\\nE2: ...\\nE3: ...\\n\\n\" + prompt\n",
    "        )\n",
    "        max_new_tokens = max(160, int(max_new_tokens * 0.85))\n",
    "        temperature = 0.1\n",
    "\n",
    "    raise RuntimeError(\"CE parse failed. Last output head:\\n\" + (last or \"\")[:800])\n",
    "\n",
    "\n",
    "\n",
    "def parse_ce_text_output(text, paper_id, topic):\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    claim_text = \"\"\n",
    "    evidences = []\n",
    "\n",
    "    for l in lines:\n",
    "        if l.startswith(\"CLAIM:\"):\n",
    "            claim_text = l.replace(\"CLAIM:\", \"\").strip()\n",
    "        elif l.startswith(\"E\") and \":\" in l:\n",
    "            evidences.append(l.split(\":\", 1)[1].strip())\n",
    "\n",
    "    while len(evidences) < 3:\n",
    "        evidences.append(\"Supports the claim.\")\n",
    "\n",
    "    return {\n",
    "        \"paper_id\": paper_id,\n",
    "        \"topic\": topic,\n",
    "        \"claim_text\": claim_text or \"Claim.\",\n",
    "        \"evidence_summaries\": evidences[:3],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def build_tree_from_ce(ce_obj, claim_quote, evidence_quotes, paper_id=None, topic=None):\n",
    "    paper_id = paper_id or ce_obj.get(\"paper_id\", \"PMID:UNKNOWN\")\n",
    "    topic = topic or ce_obj.get(\"topic\", \"UNKNOWN\")\n",
    "\n",
    "    # ✅ 防止 evidence_quotes 不足 3 条导致 IndexError\n",
    "    evidence_quotes = list(evidence_quotes or [])\n",
    "    while len(evidence_quotes) < 3:\n",
    "        evidence_quotes.append(claim_quote)\n",
    "\n",
    "    root = {\n",
    "        \"id\": \"C0\",\n",
    "        \"type\": \"claim\",\n",
    "        \"text\": ce_obj.get(\"claim_text\",\"\").strip() or claim_quote[:160],\n",
    "        \"source\": {\"paper_id\": paper_id, \"section\": \"Unknown\", \"quote\": claim_quote},\n",
    "        \"children\": []\n",
    "    }\n",
    "\n",
    "    sums = ce_obj.get(\"evidence_summaries\") or []\n",
    "    while len(sums) < 3:\n",
    "        sums.append(\"Supports the claim based on the quoted sentence.\")\n",
    "\n",
    "    for i in range(3):\n",
    "        root[\"children\"].append({\n",
    "            \"id\": f\"E{i+1}\",\n",
    "            \"type\": \"evidence\",\n",
    "            \"polarity\": \"support\",\n",
    "            \"text\": str(sums[i])[:220],\n",
    "            \"source\": {\"paper_id\": paper_id, \"section\": \"Unknown\", \"quote\": evidence_quotes[i]},\n",
    "            \"children\": []\n",
    "        })\n",
    "\n",
    "    return {\"paper_id\": paper_id, \"topic\": topic, \"root\": root}\n",
    "\n",
    "\n",
    "\n",
    "def validate_tree_min(tree):\n",
    "    r = tree.get(\"root\")\n",
    "    if not r or r.get(\"type\") != \"claim\":\n",
    "        return False, \"root invalid\"\n",
    "    if not isinstance(r.get(\"children\"), list) or len(r[\"children\"]) < 3:\n",
    "        return False, \"need >=3 evidence\"\n",
    "    return True, \"ok\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9202ba0",
   "metadata": {},
   "source": [
    "## 单篇测试：生成 argument_tree.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b94abc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT LEN: 12000 HEAD: [ABSTRACT] Critical Care April 2004 Vol 8 No 2 Prigent et al. Review Clinical review: Corticotherapy in sepsis Helene Prigent1, Virginie Maxime2 and Djillali Annane3 1Senior Resident, Service de Réani\n",
      "Claim quote: [ABSTRACT] Online ISSN 1466-609X) Abstract The use of glucocorticoids (corticotherapy) in severe sepsis is one of the main controversial issues in critical care medicine.\n",
      "Evidence 1: [ABSTRACT] These agents were commonly used to treat sepsis until the end of the 1980s, when several randomized trials casted serious doubt on any benefit from high-dose glucocorticoids.\n",
      "[CE raw head] {\n",
      "  \"CLAIM\": \"Online ISSN 1466-609X) Abstract The use of glucocorticoids (corticotherapy) in severe sepsis is one of the main controversial issues in critical care medicine.\",\n",
      "  \"E1\": \"These agents we\n",
      "validate: True ok\n",
      "saved: Annane_Sepsis_Corpus\\papers\\PMID_15025773\\argument_tree.json\n"
     ]
    }
   ],
   "source": [
    "paper_dir, obj = get_first_ready_paper()\n",
    "paper_id = f\"PMID:{obj['pmid']}\"\n",
    "topic = obj[\"topic\"]\n",
    "text = paper_text_from_sections(obj)\n",
    "\n",
    "print(\"TEXT LEN:\", len(text), \"HEAD:\", text[:200].replace(\"\\n\",\" \"))\n",
    "\n",
    "q_obj = pick_quotes_by_ids(paper_id, topic, text, max_sents=80)\n",
    "print(\"Claim quote:\", q_obj[\"claim_quote\"][:200])\n",
    "print(\"Evidence 1:\", q_obj[\"evidence_quotes\"][0][:200])\n",
    "\n",
    "ce_prompt = build_ce_from_quotes_prompt(paper_id, topic, q_obj[\"claim_quote\"], q_obj[\"evidence_quotes\"])\n",
    "raw = gemma_generate(ce_prompt, max_new_tokens=220, temperature=0.2)\n",
    "print(\"[CE raw head]\", raw[:200])\n",
    "\n",
    "ce = parse_ce_text_output(raw, paper_id, topic)\n",
    "\n",
    "tree = build_tree_from_ce(ce, q_obj[\"claim_quote\"], q_obj[\"evidence_quotes\"])\n",
    "\n",
    "ok, msg = validate_tree_min(tree)\n",
    "print(\"validate:\", ok, msg)\n",
    "\n",
    "out_path = paper_dir / \"argument_tree.json\"\n",
    "out_path.write_text(json.dumps(tree, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd13b57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CE attempt 0] head: {   \"CLAIM\": \"Online ISSN 1466-609X) Abstract The use of glucocorticoids (corticotherapy) in severe sepsis is one of the\n",
      "{'claim_text': 'Online ISSN 1466-609X) Abstract The use of glucocorticoids (corticotherapy) in severe sepsis is one of the main controversial issues in critical care medicine.', 'evidence_summaries': ['These agents were commonly used to treat sepsis until the end of the 1980s, when several randomized trials casted serious doubt on any benefit from high-dose glucocorticoids.', 'Later, important progress in our understanding of the role played by the hypothalamic–pituitary– adrenal axis in the response to sepsis, and of the mechanisms of action of glucocorticoids led us to reconsider their use in septic shock.', 'The present review summarizes the basics of the physiological response of the hypothalamic–pituitary–adrenal axis to stress, including regulation of glucocorticoid synthesis, the cellular mechanisms of action of glucocorticoids, and how they influence metabolism, cardiovascular homeostasis and the immune system.']}\n"
     ]
    }
   ],
   "source": [
    "ce = gemma_json_call_ce(ce_prompt, max_new_tokens=220, temperature=0.2, retries=3)\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10087874",
   "metadata": {},
   "source": [
    "## 批量 Step4：生成 out/argument_trees.jsonl（为每篇文章都生成argument_tree.json）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499339bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Processing: PMID:15025773\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Online ISSN 1466-609X) Abstract The use of glucocorticoids (corticotherapy) in severe sepsis is one of the ma\n",
      "Evidence 1: [ABSTRACT] These agents were commonly used to treat sepsis until the end of the 1980s, when several randomized trials ca\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Online ISSN 1466-609X) Abstract The use of glucocorticoids (corticotherapy) in severe sepsis is one of the\n",
      "✅ OK PMID:15025773\n",
      "\n",
      "============================\n",
      "Processing: PMID:23361625\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Dellinger Surviving Sepsis Campaign: International Mitchell M.\n",
      "Evidence 1: [ABSTRACT] Levy Guidelines for Management of Severe Sepsis Andrew Rhodes Djillali Annane and Septic Shock, 2012 Herwig G\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Dellinger Surviving Sepsis Campaign is a randomized, controlled trial comparing two treatment arms for pat\n",
      "✅ OK PMID:23361625\n",
      "\n",
      "============================\n",
      "Processing: PMID:26633262\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] CochraneDatabaseofSystematicReviews Corticosteroids for treating sepsis (Review) AnnaneD,BellissantE,Bollaert\n",
      "Evidence 1: [CONCLUSION] 71 Analysis1.1.Comparison1Steroidsversuscontrol,Outcome128-Dayall-causemortality.\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Cochrane Database of Systematic Reviews on corticosteroids for treating sepsis is a review of randomized c\n",
      "✅ OK PMID:26633262\n",
      "\n",
      "============================\n",
      "Processing: PMID:26903338\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] HHS Public Access Author manuscript JAMA.\n",
      "Evidence 1: [ABSTRACT] Published in final edited form as: JAMA.\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"HHS Public Access Author manuscript JAMA is a peer-reviewed medical journal published by the American Medi\n",
      "✅ OK PMID:26903338\n",
      "\n",
      "============================\n",
      "Processing: PMID:27379022\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] It is now defined as an abnormal host response to infection, resulting in life-threatening dys- function of o\n",
      "Evidence 1: [CONCLUSION] results in hypothalamic signaling via autonomic nuclei in the brainstem, which have Irreversible Damage to \n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Abstract says that an abnormal host response to infection results in life-threatening dys- function of org\n",
      "✅ OK PMID:27379022\n",
      "\n",
      "============================\n",
      "Processing: PMID:29025194\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Cochrane Library Cochrane Database of Systematic Reviews Procalcitonin to initiate or discontinue antibiotics\n",
      "Evidence 1: [ABSTRACT] Procalcitonin to initiate or discontinue antibiotics in acute respiratory tract infections.\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Procalcitonin is used in the abstract to initiate or discontinue antibiotics in acute respiratory tract in\n",
      "✅ OK PMID:29025194\n",
      "\n",
      "============================\n",
      "Processing: PMID:30105022\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Sepsis remains a major cause of morbidity and mortality.\n",
      "Evidence 1: [ABSTRACT] Treatment is nonspecific and relies on source control and organ support.\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Sepsis remains a major cause of morbidity and mortality.\",   \"E1\": \"Treatment is nonspecific and relies on\n",
      "✅ OK PMID:30105022\n",
      "\n",
      "============================\n",
      "Processing: PMID:30111341\n",
      "TEXT LEN: 9528\n",
      "Claim quote: [ABSTRACT] Meier1† , Lila Bouadma3, Charles E.\n",
      "Evidence 1: [ABSTRACT] Luyt4, Michel Wolff3, Jean Chastre4, Florence Tubach5, Stefan Schroeder6, Vandack Nobre7, Djillali Annane8, K\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Use the abstract to identify the authors and their affiliations.\",   \"E1\": \"The abstract mentions four aut\n",
      "✅ OK PMID:30111341\n",
      "\n",
      "============================\n",
      "Processing: PMID:33150472\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Chrousos4, Bram Rochwerg5,9, Amanda Busby6, Barbara Ruaro3 and Bernd Meibohm7 © 2020 This is a U.S.\n",
      "Evidence 1: [ABSTRACT] This paucity of information may have led to the heterogeneity of treatment protocols and misinterpretation of\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Chrousos4, Bram Rochwerg5,9, Amanda Busby6, Barbara Ruaro3 and Bernd Meibohm7 © 2020 This is a U.S.\",   \"E\n",
      "✅ OK PMID:33150472\n",
      "\n",
      "============================\n",
      "Processing: PMID:33159530\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Sottile10 and Laurent Papazian1,2* © 2020 Springer-Verlag GmbH Germany, part of Springer Nature Abstract Neur\n",
      "Evidence 1: [ABSTRACT] Minimization of volutrauma and ventilator-induced lung injury (VILI) results in a lower incidence of barotrau\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Sottile10 and Laurent Papazian1,2* © 2020 Springer-Verlag GmbH Germany, part of Springer Nature Abstract N\n",
      "✅ OK PMID:33159530\n",
      "\n",
      "============================\n",
      "Processing: PMID:33876268\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] Mammen3, Paul Alexander2, Zhikang Ye2, Luis Enrique Colunga Lozano2, Marie Warrer Munch4, Anders Perner4, Bin\n",
      "Evidence 1: [ABSTRACT] Pastores7, John Marshall8, François Lamontagne9, Djillali Annane10, Gianfranco Umberto Meduri11 and Bram Roch\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Mammen3, Paul Alexander2, Zhikang Ye2, Luis Enrique Colunga Lozano2, Marie Warrer Munch4, Anders Perner4, \n",
      "✅ OK PMID:33876268\n",
      "\n",
      "============================\n",
      "Processing: PMID:34351722\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] The new england journal of medicine Original Article Therapeutic Anticoagulation with Heparin in Critically I\n",
      "Evidence 1: [CONCLUSION] CONCLUSIONS In critically ill patients with Covid-19, an initial strategy of therapeutic-dose anti- coagula\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"The new england journal of medicine Original Article Therapeutic Anticoagulation with Heparin in Criticall\n",
      "✅ OK PMID:34351722\n",
      "\n",
      "============================\n",
      "Processing: PMID:35708858\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] A convergent contributor could be platelets that beyond hemostatic functions can carry infectious viruses.\n",
      "Evidence 1: [CONCLUSION] Results have so far demonstrated the presence of viral particles and their infectiousness.\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Platelets can carry infectious viruses beyond hemostatic functions\",   \"E1\": \"Results have so far demonstr\n",
      "✅ OK PMID:35708858\n",
      "\n",
      "============================\n",
      "Processing: PMID:37176780\n",
      "TEXT LEN: 10900\n",
      "Claim quote: [ABSTRACT] Corticosteroidsare powerfulanti-inflammatoryagentsthatareroutinelyusedinsepticshockandinoxygen-dependent SARS\n",
      "Evidence 1: [ABSTRACT] Corticosteroidsmaythereforealsohavearoletoplay inthetreatmentofARDS.Thisnarrativereviewwasundertakenfollowing\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Corticosteroids may also have a role to play in the treatment of ARDS. This narrative review was undertake\n",
      "✅ OK PMID:37176780\n",
      "\n",
      "============================\n",
      "Processing: PMID:37888913\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] The new england journal of medicine Original Article Simvastatin in Critically Ill Patients with Covid-19 The\n",
      "Evidence 1: [METHOD] METHODS of the members of the writing committee In an ongoing international, multifactorial, adaptive platform,\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"The new england journal of medicine Original Article Simvastatin in Critically Ill Patients with Covid-19 \n",
      "✅ OK PMID:37888913\n",
      "\n",
      "============================\n",
      "Processing: PMID:38250247\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] LATE BREAKER ARTICLE Corticosteroids in Sepsis and Septic Shock: A Systematic Review, Pairwise, and Dose-Resp\n",
      "Evidence 1: [METHOD] METHODS We registered a protocol on Open Science Framework Search Strategy and Study Selection in accordance wi\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"该研究是关于 Sepsis 和 Septic Shock 中 Corticosteroid 的系统性回顾和分析。\",   \"E1\": \"该研究通过注册在 Open Science Framework Search\n",
      "✅ OK PMID:38250247\n",
      "\n",
      "============================\n",
      "Processing: PMID:39787606\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] ORIGINAL ARTICLE Status of Sepsis Care in European Hospitals Results from an International Cross-Sectional Su\n",
      "Evidence 1: [ABSTRACT] Giamarellos-Bourboulis2, Ricard Ferrer3, Evgeny A.\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Original article discusses the status of sepsis care in European hospitals.\",   \"E1\": \"The abstract provid\n",
      "✅ OK PMID:39787606\n",
      "\n",
      "============================\n",
      "Processing: PMID:39905519\n",
      "TEXT LEN: 12000\n",
      "Claim quote: [ABSTRACT] NETs physically trap and kill pathogens as well as activating and facilitating crosstalk between immune cells\n",
      "Evidence 1: [ABSTRACT] Exces- sive or inadequately resolved NETs are implicated in the underlying pathophysiology of sepsis and othe\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"NETs physically trap and kill pathogens as well as activating and facilitating crosstalk between immune ce\n",
      "✅ OK PMID:39905519\n",
      "\n",
      "============================\n",
      "Processing: PMID:41180093\n",
      "TEXT LEN: 11932\n",
      "Claim quote: [ABSTRACT] Although international guidelines have helped reduce crude mortality rates from sepsis by optimizing infectio\n",
      "Evidence 1: [ABSTRACT] The aims of this narrative review were Pharmacology to provide readers with the most recent data on corticost\n",
      "[CE attempt 0] head: {   \"CLAIM\": \"Although international guidelines have helped reduce crude mortality rates from sepsis by optimizing infec\n",
      "✅ OK PMID:41180093\n",
      "\n",
      "DONE. OK: 19 FAIL: 0\n",
      "WROTE: Annane_Sepsis_Corpus\\out\\argument_trees.jsonl\n"
     ]
    }
   ],
   "source": [
    "OUT_JSONL = OUT_DIR / \"argument_trees.jsonl\"\n",
    "\n",
    "def iter_ready_papers():\n",
    "    for p in sorted(PAPERS_DIR.glob(\"PMID_*\")):\n",
    "        sec = p / \"sections.json\"\n",
    "        if not sec.exists():\n",
    "            continue\n",
    "        obj = json.loads(sec.read_text(encoding=\"utf-8\"))\n",
    "        text = paper_text_from_sections(obj, max_chars=12000)\n",
    "        if len(text) < 400:\n",
    "            continue\n",
    "        yield p, obj, text\n",
    "\n",
    "ok_cnt, fail_cnt = 0, 0\n",
    "\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for paper_dir, obj, text in iter_ready_papers():\n",
    "        paper_id = f\"PMID:{obj['pmid']}\"\n",
    "        topic = obj[\"topic\"]\n",
    "\n",
    "        print(\"\\n\" + \"=\"*28)\n",
    "        print(\"Processing:\", paper_id)\n",
    "        print(\"TEXT LEN:\", len(text))\n",
    "\n",
    "        try:\n",
    "            # 1) 选 quote\n",
    "            q_obj = pick_quotes_by_ids(paper_id, topic, text, max_sents=80)\n",
    "            print(\"Claim quote:\", q_obj[\"claim_quote\"][:120])\n",
    "            print(\"Evidence 1:\", q_obj[\"evidence_quotes\"][0][:120])\n",
    "\n",
    "            # 2) quote -> claim/evidence summaries\n",
    "            ce_prompt = build_ce_from_quotes_prompt(\n",
    "                paper_id, topic,\n",
    "                q_obj[\"claim_quote\"],\n",
    "                q_obj[\"evidence_quotes\"]\n",
    "            )\n",
    "\n",
    "            ce = gemma_json_call_ce(ce_prompt, max_new_tokens=220, temperature=0.2, retries=3)\n",
    "\n",
    "            # ✅ 关键修改：补齐 paper_id/topic，避免 KeyError\n",
    "            ce[\"paper_id\"] = paper_id\n",
    "            ce[\"topic\"] = topic\n",
    "\n",
    "            # 3) 拼树（写入 quote）\n",
    "            #tree = build_tree_from_ce(ce, q_obj[\"claim_quote\"], q_obj[\"evidence_quotes\"])\n",
    "            tree = build_tree_from_ce(ce, q_obj[\"claim_quote\"], q_obj[\"evidence_quotes\"], paper_id=paper_id, topic=topic)\n",
    "\n",
    "\n",
    "            ok2, msg2 = validate_tree_min(tree)\n",
    "            if not ok2:\n",
    "                raise ValueError(msg2)\n",
    "\n",
    "            (paper_dir / \"argument_tree.json\").write_text(\n",
    "                json.dumps(tree, ensure_ascii=False, indent=2),\n",
    "                encoding=\"utf-8\"\n",
    "            )\n",
    "            f.write(json.dumps(tree, ensure_ascii=False) + \"\\n\")\n",
    "            f.flush()\n",
    "\n",
    "            ok_cnt += 1\n",
    "            print(\"✅ OK\", paper_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            fail_cnt += 1\n",
    "            print(\"❌ FAIL\", paper_id, \"->\", repr(e))\n",
    "\n",
    "print(\"\\nDONE. OK:\", ok_cnt, \"FAIL:\", fail_cnt)\n",
    "print(\"WROTE:\", OUT_JSONL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544976a",
   "metadata": {},
   "source": [
    "## Step5：jsonl → nodes.tsv / edges.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "616d03b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 74 edges: 55\n",
      "wrote: Annane_Sepsis_Corpus\\out\\argument_nodes.tsv\n",
      "wrote: Annane_Sepsis_Corpus\\out\\argument_edges.tsv\n",
      "noise evidence removed: 2\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "import pandas as pd\n",
    "\n",
    "OUT_NODES = OUT_DIR / \"argument_nodes.tsv\"\n",
    "OUT_EDGES = OUT_DIR / \"argument_edges.tsv\"\n",
    "\n",
    "# ---------- 1) 抽 section：优先用 source.section；没有就从 quote 前缀推断 ----------\n",
    "def infer_section(node):\n",
    "    src = node.get(\"source\") or {}\n",
    "    sec = (src.get(\"section\") or \"\").strip()\n",
    "    if sec and sec.lower() != \"unknown\":\n",
    "        return sec\n",
    "\n",
    "    q = (src.get(\"quote\") or \"\").strip()\n",
    "    m = re.match(r\"^\\[(ABSTRACT|METHOD|CONCLUSION|RESULTS|INTRODUCTION|DISCUSSION)\\]\\s*\", q, re.I)\n",
    "    if m:\n",
    "        key = m.group(1).lower()\n",
    "        return {\n",
    "            \"abstract\": \"Abstract\",\n",
    "            \"method\": \"Method\",\n",
    "            \"conclusion\": \"Conclusion\",\n",
    "            \"results\": \"Results\",\n",
    "            \"introduction\": \"Introduction\",\n",
    "            \"discussion\": \"Discussion\",\n",
    "        }.get(key, \"Unknown\")\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# ---------- 2) 噪声过滤：去掉明显“元信息”句子 ----------\n",
    "NOISE_PATTERNS = [\n",
    "    r\"\\bISSN\\b\", r\"\\bdoi\\b\", r\"\\bhttp\\b\", r\"\\bwww\\b\",\n",
    "    r\"author manuscript\", r\"public access\", r\"published in final edited form\",\n",
    "    r\"\\bcopyright\\b\", r\"©\", r\"springer\", r\"wiley\", r\"elsevier\",\n",
    "    r\"cochrane\", r\"table of contents\", r\"guidelines committee\",\n",
    "    r\"all rights reserved\", r\"\\blicense\\b\",\n",
    "]\n",
    "NOISE_RE = re.compile(\"|\".join(f\"(?:{p})\" for p in NOISE_PATTERNS), re.I)\n",
    "\n",
    "def is_noise_text(s: str) -> bool:\n",
    "    if not s:\n",
    "        return True\n",
    "    t = s.strip()\n",
    "    if len(t) < 40:                      # 太短，通常是标题碎片\n",
    "        return True\n",
    "    if NOISE_RE.search(t):               # 命中明显出版元信息\n",
    "        return True\n",
    "    # 名字/作者列表特征：逗号很多 + 大写首字母很多\n",
    "    comma_cnt = t.count(\",\")\n",
    "    if comma_cnt >= 6:\n",
    "        return True\n",
    "    cap_words = sum(1 for w in re.findall(r\"\\b[A-Z][a-z]+\\b\", t))\n",
    "    if cap_words >= 12 and comma_cnt >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ---------- 3) walk_tree：一边扁平化，一边清洗 ----------\n",
    "def walk_tree(node, paper_id, parent_id=None, nodes=None, edges=None):\n",
    "    if nodes is None: nodes = []\n",
    "    if edges is None: edges = []\n",
    "\n",
    "    nid = node[\"id\"]\n",
    "    ntype = node.get(\"type\") or \"unknown\"\n",
    "    text = (node.get(\"text\") or \"\").strip()\n",
    "    quote = ((node.get(\"source\") or {}).get(\"quote\") or \"\").strip()\n",
    "    section = infer_section(node)\n",
    "\n",
    "    # ✅ 内容清洗：如果 text 太模板/太空，用 quote 替代一部分\n",
    "    if not text or text.lower() in {\"claim.\", \"supports the claim.\", \"evidence\"}:\n",
    "        # 取 quote 去掉 [SECTION] 前缀\n",
    "        text2 = re.sub(r\"^\\[[A-Z]+\\]\\s*\", \"\", quote).strip()\n",
    "        text = text2[:220] if text2 else text\n",
    "\n",
    "    # ✅ 节点是否有效（噪声就标记，后面可选择丢掉）\n",
    "    node_is_noise = is_noise_text(text) and is_noise_text(quote)\n",
    "\n",
    "    nodes.append({\n",
    "        \"paper_id\": paper_id,\n",
    "        \"id\": nid,\n",
    "        \"type\": ntype,\n",
    "        \"text\": text,\n",
    "        \"section\": section,\n",
    "        \"quote\": quote,\n",
    "        \"is_noise\": int(node_is_noise),\n",
    "    })\n",
    "\n",
    "    if parent_id is not None:\n",
    "        edges.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            # 先保持“树结构边”：parent -> child\n",
    "            \"subj\": parent_id,\n",
    "            \"pred\": node.get(\"polarity\") or \"support\",\n",
    "            \"obj\": nid\n",
    "        })\n",
    "\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        walk_tree(ch, paper_id, parent_id=nid, nodes=nodes, edges=edges)\n",
    "\n",
    "    return nodes, edges\n",
    "\n",
    "# ---------- 4) 读取 JSONL ----------\n",
    "all_nodes, all_edges = [], []\n",
    "\n",
    "with OUT_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        tree = json.loads(line)\n",
    "        paper_id = tree.get(\"paper_id\", \"PMID:UNKNOWN\")\n",
    "        n, e = walk_tree(tree[\"root\"], paper_id)\n",
    "        all_nodes.extend(n)\n",
    "        all_edges.extend(e)\n",
    "\n",
    "nodes_df = pd.DataFrame(all_nodes).drop_duplicates(subset=[\"paper_id\",\"id\"])\n",
    "edges_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# ---------- 5) 强制剔除“噪声 evidence”，并同步删边 ----------\n",
    "# 只删 evidence 噪声；claim（C0）即使噪声也先保留，避免全空\n",
    "noise_evidence = nodes_df[(nodes_df[\"type\"] == \"evidence\") & (nodes_df[\"is_noise\"] == 1)]\n",
    "bad_ids = set(zip(noise_evidence[\"paper_id\"], noise_evidence[\"id\"]))\n",
    "\n",
    "if bad_ids:\n",
    "    # 删除这些 evidence 节点\n",
    "    nodes_df = nodes_df[~nodes_df.apply(lambda r: (r[\"paper_id\"], r[\"id\"]) in bad_ids, axis=1)]\n",
    "    # 删除与之相关的边\n",
    "    edges_df = edges_df[~edges_df.apply(lambda r: (r[\"paper_id\"], r[\"obj\"]) in bad_ids, axis=1)]\n",
    "\n",
    "# 再保证：每篇至少保留 1 个 evidence，不足则不删（安全兜底）\n",
    "def ensure_min_evidence(df_nodes, df_edges):\n",
    "    kept = []\n",
    "    for pid, grp in df_nodes.groupby(\"paper_id\"):\n",
    "        evid = grp[grp[\"type\"] == \"evidence\"]\n",
    "        if len(evid) == 0:\n",
    "            # 兜底：不做进一步处理，需要回去检查该 paper 的抽取质量\n",
    "            kept.append(pid)\n",
    "    return kept\n",
    "\n",
    "no_evidence_papers = ensure_min_evidence(nodes_df, edges_df)\n",
    "if no_evidence_papers:\n",
    "    print(\"⚠️ papers with 0 evidence after filtering:\", no_evidence_papers[:5], \"...\" if len(no_evidence_papers)>5 else \"\")\n",
    "\n",
    "# ---------- 6) 写出 ----------\n",
    "nodes_df.to_csv(OUT_NODES, sep=\"\\t\", index=False)\n",
    "edges_df.to_csv(OUT_EDGES, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"nodes:\", len(nodes_df), \"edges:\", len(edges_df))\n",
    "print(\"wrote:\", OUT_NODES)\n",
    "print(\"wrote:\", OUT_EDGES)\n",
    "print(\"noise evidence removed:\", len(bad_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67218591",
   "metadata": {},
   "source": [
    "## Step6：导出 RDF Turtle（TTL），用于截图的 RDF 可视化工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec88c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote MIN TTL (for visualization): Annane_Sepsis_Corpus\\out\\argument_graph_min.ttl\n",
      "Wrote FULL TTL (for storage/search): Annane_Sepsis_Corpus\\out\\argument_graph_full.ttl\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "TTL_MIN = OUT_DIR / \"argument_graph_min.ttl\"\n",
    "TTL_FULL = OUT_DIR / \"argument_graph_full.ttl\"\n",
    "\n",
    "def esc(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "    s = s.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\")\n",
    "    return s\n",
    "\n",
    "def safe_id(s: str):\n",
    "    return re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(s))\n",
    "\n",
    "def node_uri(paper_id, nid):\n",
    "    pid = safe_id(paper_id.replace(\":\", \"\"))\n",
    "    nid2 = safe_id(nid)\n",
    "    return f\"ex:{pid}_{nid2}\"\n",
    "\n",
    "def infer_section_from_quote(q: str):\n",
    "    if not q:\n",
    "        return \"Unknown\"\n",
    "    m = re.match(r\"^\\[(ABSTRACT|METHOD|CONCLUSION|RESULTS|INTRODUCTION|DISCUSSION)\\]\\s*\", q.strip(), re.I)\n",
    "    if not m:\n",
    "        return \"Unknown\"\n",
    "    key = m.group(1).lower()\n",
    "    return {\n",
    "        \"abstract\": \"Abstract\",\n",
    "        \"method\": \"Method\",\n",
    "        \"conclusion\": \"Conclusion\",\n",
    "        \"results\": \"Results\",\n",
    "        \"introduction\": \"Introduction\",\n",
    "        \"discussion\": \"Discussion\",\n",
    "    }.get(key, \"Unknown\")\n",
    "\n",
    "def short_label(node, paper_id):\n",
    "    \"\"\"图上显示用：尽量短，不要 quote 全文\"\"\"\n",
    "    nid = node.get(\"id\", \"\")\n",
    "    t = (node.get(\"type\") or \"\").lower()\n",
    "    src = node.get(\"source\") or {}\n",
    "    q = (src.get(\"quote\") or \"\").strip()\n",
    "    sec = (src.get(\"section\") or \"\").strip()\n",
    "    if not sec or sec.lower() == \"unknown\":\n",
    "        sec = infer_section_from_quote(q)\n",
    "\n",
    "    # 用 id + section 做 label，别放长文本\n",
    "    if t == \"claim\":\n",
    "        return f\"{paper_id} C0 ({sec})\"\n",
    "    else:\n",
    "        return f\"{paper_id} {nid} ({sec})\"\n",
    "\n",
    "# --------- 核心：只画关系，不画属性节点 ---------\n",
    "def walk_emit(node, paper_id, triples_min, triples_full, parent_uri=None):\n",
    "    nid = node.get(\"id\")\n",
    "    nuri = node_uri(paper_id, nid)\n",
    "\n",
    "    ntype = (node.get(\"type\") or \"evidence\").lower()\n",
    "    rdf_type = \"ex:Claim\" if ntype == \"claim\" else \"ex:Evidence\"\n",
    "\n",
    "    # ---- MIN：只有 type + label + supports/attacks ----\n",
    "    triples_min.append(f'{nuri} rdf:type {rdf_type} .')\n",
    "    triples_min.append(f'{nuri} rdfs:label \"{esc(short_label(node, paper_id))}\" .')\n",
    "\n",
    "    # ---- FULL：保留元信息，给后续用（不建议拿来直接画图）----\n",
    "    triples_full.append(f'{nuri} rdf:type {rdf_type} .')\n",
    "    triples_full.append(f'{nuri} ex:paper_id \"{esc(paper_id)}\" .')\n",
    "    triples_full.append(f'{nuri} ex:node_id \"{esc(nid)}\" .')\n",
    "\n",
    "    src = node.get(\"source\") or {}\n",
    "    quote = (src.get(\"quote\") or \"\").strip()\n",
    "    section = (src.get(\"section\") or \"\").strip()\n",
    "    if not section or section.lower() == \"unknown\":\n",
    "        section = infer_section_from_quote(quote)\n",
    "\n",
    "    text = (node.get(\"text\") or \"\").strip()\n",
    "    triples_full.append(f'{nuri} ex:section \"{esc(section)}\" .')\n",
    "    triples_full.append(f'{nuri} ex:text \"{esc(text)}\" .')\n",
    "    triples_full.append(f'{nuri} ex:quote \"{esc(quote)}\" .')\n",
    "\n",
    "    # ✅ 关键：关系方向：child -> parent（Evidence supports/attacks Claim）\n",
    "    if parent_uri is not None:\n",
    "        pol = (node.get(\"polarity\") or \"support\").lower()\n",
    "        pred = \"ex:attacks\" if pol == \"attack\" else \"ex:supports\"\n",
    "        triples_min.append(f\"{nuri} {pred} {parent_uri} .\")\n",
    "        triples_full.append(f\"{nuri} {pred} {parent_uri} .\")\n",
    "\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        walk_emit(ch, paper_id, triples_min, triples_full, parent_uri=nuri)\n",
    "\n",
    "# --------- 写文件 ---------\n",
    "prefix = [\n",
    "    '@prefix ex: <http://example.org/sepsis/> .',\n",
    "    '@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .',\n",
    "    '@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .',\n",
    "    \"\"\n",
    "]\n",
    "triples_min = prefix.copy()\n",
    "triples_full = prefix.copy()\n",
    "\n",
    "with OUT_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        tree = json.loads(line)\n",
    "        paper_id = tree.get(\"paper_id\", \"PMID:UNKNOWN\")\n",
    "        walk_emit(tree[\"root\"], paper_id, triples_min, triples_full, parent_uri=None)\n",
    "        triples_min.append(\"\")\n",
    "        triples_full.append(\"\")\n",
    "\n",
    "TTL_MIN.write_text(\"\\n\".join(triples_min), encoding=\"utf-8\")\n",
    "TTL_FULL.write_text(\"\\n\".join(triples_full), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote MIN TTL (for visualization):\", TTL_MIN)\n",
    "print(\"Wrote FULL TTL (for storage/search):\", TTL_FULL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b93575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote TTL: Annane_Sepsis_Corpus\\out\\argument_graph.ttl\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "TTL_PATH = OUT_DIR / \"argument_graph.ttl\"\n",
    "\n",
    "def esc(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "    s = s.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\")\n",
    "    return s\n",
    "\n",
    "def safe_id(s: str):\n",
    "    # TTL 的 localName 最好只用字母数字下划线\n",
    "    return re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(s))\n",
    "\n",
    "def node_uri(paper_id, nid):\n",
    "    pid = safe_id(paper_id.replace(\":\", \"\"))\n",
    "    nid2 = safe_id(nid)\n",
    "    return f\"ex:{pid}_{nid2}\"\n",
    "\n",
    "# --------- section 兜底：优先 source.section，否则从 quote 的 [ABSTRACT] 推断 ----------\n",
    "def infer_section(node):\n",
    "    src = node.get(\"source\") or {}\n",
    "    sec = (src.get(\"section\") or \"\").strip()\n",
    "    if sec and sec.lower() != \"unknown\":\n",
    "        return sec\n",
    "    q = (src.get(\"quote\") or \"\").strip()\n",
    "    m = re.match(r\"^\\[(ABSTRACT|METHOD|CONCLUSION|RESULTS|INTRODUCTION|DISCUSSION)\\]\\s*\", q, re.I)\n",
    "    if m:\n",
    "        key = m.group(1).lower()\n",
    "        return {\n",
    "            \"abstract\": \"Abstract\",\n",
    "            \"method\": \"Method\",\n",
    "            \"conclusion\": \"Conclusion\",\n",
    "            \"results\": \"Results\",\n",
    "            \"introduction\": \"Introduction\",\n",
    "            \"discussion\": \"Discussion\",\n",
    "        }.get(key, \"Unknown\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# --------- 噪声过滤（和你 nodes/edges 的过滤逻辑保持一致） ----------\n",
    "NOISE_PATTERNS = [\n",
    "    r\"\\bISSN\\b\", r\"\\bdoi\\b\", r\"\\bhttp\\b\", r\"\\bwww\\b\",\n",
    "    r\"author manuscript\", r\"public access\", r\"published in final edited form\",\n",
    "    r\"\\bcopyright\\b\", r\"©\", r\"springer\", r\"wiley\", r\"elsevier\",\n",
    "    r\"cochrane\", r\"table of contents\", r\"guidelines committee\",\n",
    "    r\"all rights reserved\", r\"\\blicense\\b\",\n",
    "]\n",
    "NOISE_RE = re.compile(\"|\".join(f\"(?:{p})\" for p in NOISE_PATTERNS), re.I)\n",
    "\n",
    "def is_noise_text(s: str) -> bool:\n",
    "    if not s:\n",
    "        return True\n",
    "    t = s.strip()\n",
    "    if len(t) < 40:\n",
    "        return True\n",
    "    if NOISE_RE.search(t):\n",
    "        return True\n",
    "    comma_cnt = t.count(\",\")\n",
    "    if comma_cnt >= 6:\n",
    "        return True\n",
    "    cap_words = sum(1 for w in re.findall(r\"\\b[A-Z][a-z]+\\b\", t))\n",
    "    if cap_words >= 12 and comma_cnt >= 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def node_is_noise(node):\n",
    "    text = (node.get(\"text\") or \"\").strip()\n",
    "    quote = ((node.get(\"source\") or {}).get(\"quote\") or \"\").strip()\n",
    "    # 只有 evidence 才过滤（claim 先保留避免全空）\n",
    "    if (node.get(\"type\") or \"\").lower() == \"evidence\":\n",
    "        return is_noise_text(text) and is_noise_text(quote)\n",
    "    return False\n",
    "\n",
    "# --------- 正确语义：Evidence supports Claim（child -> parent） ----------\n",
    "def walk_emit(node, paper_id, triples, parent_uri=None, parent_type=None):\n",
    "    nid = node.get(\"id\")\n",
    "    nuri = node_uri(paper_id, nid)\n",
    "\n",
    "    ntype = (node.get(\"type\") or \"unknown\").lower()\n",
    "    rdf_type = \"ex:Claim\" if ntype == \"claim\" else \"ex:Evidence\"\n",
    "    triples.append(f'{nuri} rdf:type {rdf_type} .')\n",
    "\n",
    "    triples.append(f'{nuri} ex:paper_id \"{esc(paper_id)}\" .')\n",
    "    triples.append(f'{nuri} ex:node_id \"{esc(nid)}\" .')\n",
    "\n",
    "    # text：如果是模板，尽量用 quote 补\n",
    "    text = (node.get(\"text\") or \"\").strip()\n",
    "    src = node.get(\"source\") or {}\n",
    "    quote = (src.get(\"quote\") or \"\").strip()\n",
    "\n",
    "    if not text or text.lower() in {\"claim.\", \"supports the claim.\", \"supports the claim\", \"evidence\"}:\n",
    "        text2 = re.sub(r\"^\\[[A-Z]+\\]\\s*\", \"\", quote).strip()\n",
    "        if text2:\n",
    "            text = text2[:220]\n",
    "\n",
    "    triples.append(f'{nuri} ex:text \"{esc(text)}\" .')\n",
    "    triples.append(f'{nuri} ex:section \"{esc(infer_section(node))}\" .')\n",
    "    triples.append(f'{nuri} ex:quote \"{esc(quote)}\" .')\n",
    "\n",
    "    # ✅ 关键：把关系方向改为 child -> parent（Evidence supports Claim）\n",
    "    if parent_uri is not None:\n",
    "        pol = (node.get(\"polarity\") or \"support\").lower()\n",
    "        # 只在非噪声 evidence 上建边（避免作者/ISSN 垃圾边）\n",
    "        if not node_is_noise(node):\n",
    "            if pol == \"attack\":\n",
    "                triples.append(f'{nuri} ex:attacks {parent_uri} .')\n",
    "            else:\n",
    "                triples.append(f'{nuri} ex:supports {parent_uri} .')\n",
    "\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        walk_emit(ch, paper_id, triples, parent_uri=nuri, parent_type=ntype)\n",
    "\n",
    "# --------- 写 TTL ----------\n",
    "triples = [\n",
    "    '@prefix ex: <http://example.org/sepsis/> .',\n",
    "    '@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .',\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "with OUT_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        tree = json.loads(line)\n",
    "        paper_id = tree.get(\"paper_id\", \"PMID:UNKNOWN\")\n",
    "        walk_emit(tree[\"root\"], paper_id, triples, parent_uri=None)\n",
    "        triples.append(\"\")\n",
    "\n",
    "TTL_PATH.write_text(\"\\n\".join(triples), encoding=\"utf-8\")\n",
    "print(\"Wrote TTL:\", TTL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ba631",
   "metadata": {},
   "source": [
    "## 下一步：你要的“嵌套 support/attack”\n",
    "你本周的图二最终希望 evidence 下还有 support/attack evidence。\n",
    "下面先给你一个“深度=2”的扩展骨架：对每条 evidence 再挑 2 条句子（支持/反驳）。你确认后我再帮你把 prompt 调到稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3421af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_child_ids_prompt(paper_id, topic, evidence_quote, indexed_sents_text):\n",
    "    return f\"\"\"\n",
    "你必须只输出严格合法 JSON，不要输出解释。\n",
    "现在我们要为下面这条 evidence 找：\n",
    "- 1条支持它的句子编号 support_id\n",
    "- 1条反驳它的句子编号 attack_id\n",
    "\n",
    "输出格式：\n",
    "{{\n",
    "  \"support_id\": 1,\n",
    "  \"attack_id\": 2\n",
    "}}\n",
    "\n",
    "evidence_quote:\n",
    "{evidence_quote}\n",
    "\n",
    "编号句子：\n",
    "{indexed_sents_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "def add_nested_children_depth2(tree, text, max_sents=80):\n",
    "    sents, indexed = build_indexed_sentences(text, max_sents=max_sents)\n",
    "\n",
    "    for ev in tree[\"root\"][\"children\"]:\n",
    "        ev_quote = (ev.get(\"source\") or {}).get(\"quote\",\"\")\n",
    "        prompt = build_child_ids_prompt(tree[\"paper_id\"], tree[\"topic\"], ev_quote, indexed)\n",
    "        raw = gemma_generate(prompt, max_new_tokens=120, temperature=0.1)\n",
    "        obj = extract_json_loose(raw)\n",
    "\n",
    "        sid = max(1, min(int(obj[\"support_id\"]), len(sents)))\n",
    "        aid = max(1, min(int(obj[\"attack_id\"]), len(sents)))\n",
    "\n",
    "        ev[\"children\"] = [\n",
    "            {\n",
    "                \"id\": f\"{ev['id']}_S1\",\n",
    "                \"type\": \"evidence\",\n",
    "                \"polarity\": \"support\",\n",
    "                \"text\": sents[sid-1][:120],\n",
    "                \"source\": {\"paper_id\": tree[\"paper_id\"], \"section\": \"Unknown\", \"quote\": sents[sid-1]},\n",
    "                \"children\": []\n",
    "            },\n",
    "            {\n",
    "                \"id\": f\"{ev['id']}_A1\",\n",
    "                \"type\": \"evidence\",\n",
    "                \"polarity\": \"attack\",\n",
    "                \"text\": sents[aid-1][:120],\n",
    "                \"source\": {\"paper_id\": tree[\"paper_id\"], \"section\": \"Unknown\", \"quote\": sents[aid-1]},\n",
    "                \"children\": []\n",
    "            }\n",
    "        ]\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af496cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fdf6e74",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
